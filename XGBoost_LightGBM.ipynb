{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Machine Learning Algorithms\n",
    "\n",
    "Gradient boosting decision tree (**GBDT**) is one of the best performing classes of algorithms in machine learning competitions. One implementation of the gradient boosting decision tree – xgboost – is one of the most popular algorithms on Kaggle. Among the 29 challenge winning solutions published at Kaggle’s blog during 2015, 17 used xgboost. If you take a look at the kernels in a Kaggle competition, you can clearly see how popular xgboost is.\n",
    "\n",
    "Though xgboost seemed to be the go-to algorithm in Kaggle for a while, a new contender is quickly gaining traction: lightGBM. Released from Microsoft, this algorithm has been claimed to be more efficient (better predictive performance for the same running time) than xgboost.\n",
    "\n",
    "One thing that can be confusing is the difference between xgboost, lightGBM and Gradient Boosting Decision Trees (which we will henceforth refer to as GBDTs). Xgboost and lightGBM are both subtypes/specific instances of the GBDT algorithm. Though they both implement the same underlying algorithm, they each introduce various tricks to make training more efficient or to improve performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics of Boosted Decision Trees\n",
    "\n",
    "Gradient boosting is a machine learning technique that produces a prediction model in the form of an ensemble of weak classifiers, optimizing a differentiable loss function. One of the most popular types of gradient boosting is boosted decision trees, that internally is made up of an ensemble of weak decision trees. There are two different strategies to compute the trees: level-wise and leaf-wise. The level-wise strategy grows the tree level by level. In this strategy, each node splits the data prioritizing the nodes closer to the tree root. The leaf-wise strategy grows the tree by splitting the data at the nodes with the highest loss change. Level-wise growth is usually better for smaller datasets whereas leaf-wise tends to overfit. Leaf-wise growth tends to excel in larger datasets where it is considerably faster than level-wise growth.\n",
    "\n",
    "A key challenge in training boosted decision trees is the **computational cost of finding the best split for each leaf**. Conventional techniques find the **exact split** for each leaf, and require scanning through all the data in each iteration. A different approach **approximates the split** by building histograms of the features. That way, the algorithm doesn’t need to evaluate every single value of the features to compute the split, but only the bins of the histogram, which are bounded. This approach turns out to be much more efficient for large datasets, without adversely affecting accuracy.\n",
    "\n",
    "**XGBoost started in 2014**, and it has become popular due to its use in many **winning Kaggle competition entries**. Originally XGBoost was based on a level-wise growth algorithm, but recently has added an option for leaf-wise growth that implements split approximation using histograms. We refer to this version as XGBoost hist. LightGBM is a more recent arrival, started in March 2016 and open-sourced in August 2016. It is based on a leaf-wise algorithm and histogram approximation, and has attracted a lot of attention due to its speed. Apart from multithreaded CPU implementations, GPU acceleration is now available on both XGBoost and LightGBM too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation on a Dataset\n",
    "I am using the Kaggle Dataset of flight delays for the year 2015 as it has both categorical and numerical features. With approximately 5 million rows, this dataset will be good for judging the performance in terms of both speed and accuracy of tuned models for each type of boosting. I will be using a 10% subset of this data ~ 500k rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"flights.csv\")\n",
    "data = data.sample(frac = 0.1, random_state=10)\n",
    "\n",
    "data = data[[\"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\"AIRLINE\",\"FLIGHT_NUMBER\",\"DESTINATION_AIRPORT\", \"ORIGIN_AIRPORT\",\n",
    "             \"AIR_TIME\", \"DEPARTURE_TIME\",\"DISTANCE\",\"ARRIVAL_DELAY\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ARRIVAL_DELAY\"] = (data[\"ARRIVAL_DELAY\"]>10)*1\n",
    "\n",
    "cols = [\"AIRLINE\",\"FLIGHT_NUMBER\",\"DESTINATION_AIRPORT\",\"ORIGIN_AIRPORT\"]\n",
    "for item in cols:\n",
    "    data[item] = data[item].astype(\"category\").cat.codes +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop([\"ARRIVAL_DELAY\"], axis=1), \n",
    "        data[\"ARRIVAL_DELAY\"], random_state=10, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receiver operating characteristic (ROC) \n",
    "# Based on the ROC curve, we can then compute the so-called ROC area under\n",
    "# the curve (ROC auc) to characterize the performance of a classification model.\n",
    "def auc(m, X_train, X_test): \n",
    "    return (metrics.roc_auc_score(y_train,m.predict_proba(X_train)[:,1]),\n",
    "                            metrics.roc_auc_score(y_test,m.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Tuning\n",
    "model = xgb.XGBClassifier()\n",
    "param_dist = {\"max_depth\": [10,30,50],\n",
    "              \"min_child_weight\" : [1,3,6],\n",
    "              \"n_estimators\": [200],\n",
    "              \"learning_rate\": [0.05, 0.1,0.16],}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid=param_dist, cv = 3,                                  verbose=10, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(max_depth=50, min_child_weight=1, n_estimators=200, n_jobs=-1 , verbose=1,learning_rate=0.16)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "auc(model, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def auc2(m, train, test): \n",
    "    return (metrics.roc_auc_score(y_train,m.predict(train)),\n",
    "                            metrics.roc_auc_score(y_test,m.predict(test)))\n",
    "\n",
    "lg = lgb.LGBMClassifier(silent=False)\n",
    "param_dist = {\"max_depth\": [25,50, 75],\n",
    "              \"learning_rate\" : [0.01,0.05,0.1],\n",
    "              \"num_leaves\": [300,900,1200],\n",
    "              \"n_estimators\": [200]\n",
    "             }\n",
    "grid_search = GridSearchCV(lg, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"roc_auc\", verbose=5)\n",
    "grid_search.fit(X_train,y_train)\n",
    "grid_search.best_estimator_\n",
    "\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "params = {\"max_depth\": 50, \"learning_rate\" : 0.1, \"num_leaves\": 900,  \"n_estimators\": 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Categorical Features\n",
    "model2 = lgb.train(params, d_train)\n",
    "auc2(model2, X_train, X_test)\n",
    "\n",
    "# With Catgeorical Features\n",
    "cate_features_name = [\"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\"AIRLINE\",\"DESTINATION_AIRPORT\",\n",
    "                 \"ORIGIN_AIRPORT\"]\n",
    "model2 = lgb.train(params, d_train, categorical_feature = cate_features_name)\n",
    "auc2(model2, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
